# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„SFTåˆ°DPOæ•°æ®é›†è½¬æ¢å™¨
æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå†…å­˜ä¼˜åŒ–åŠŸèƒ½
"""

import json
import os
import time
from typing import List, Dict, Any, Optional
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

from .sft_to_dpo_converter import SFTToDPOConverter
from ..data_loader import DataLoader
from ..model_caller import ModelCaller

# å°è¯•å¯¼å…¥streamlitï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨None
try:
    import streamlit as st
except ImportError:
    st = None


class OptimizedSFTToDPOConverter(SFTToDPOConverter):
    """
    ä¼˜åŒ–çš„SFTåˆ°DPOæ•°æ®é›†è½¬æ¢å™¨
    
    åœ¨åŸæœ‰è½¬æ¢å™¨åŸºç¡€ä¸Šå¢åŠ ï¼š
    - æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
    - å†…å­˜ä¼˜åŒ–ï¼ˆå®æ—¶ä¿å­˜ï¼‰
    - æ›´è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ª
    - é”™è¯¯æ¢å¤æœºåˆ¶
    """
    
    def __init__(
        self,
        model_caller: ModelCaller,
        data_loader: DataLoader,
        rejected_prompt: str,
        checkpoint_dir: str = "checkpoints",
        sample_min: int = 3,
        sample_max: int = 6
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„SFTåˆ°DPOè½¬æ¢å™¨
        
        Args:
            model_caller: æ¨¡å‹è°ƒç”¨å™¨
            data_loader: æ•°æ®åŠ è½½å™¨
            rejected_prompt: ç”Ÿæˆrejectedçš„æç¤ºæ¨¡æ¿
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
            sample_min: æœ€å°‘ç¤ºä¾‹æ•°é‡
            sample_max: æœ€å¤šç¤ºä¾‹æ•°é‡
        """
        super().__init__(model_caller, data_loader, rejected_prompt, sample_min, sample_max)
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)
    
    def _get_checkpoint_path(self, output_file: str) -> str:
        """è·å–æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„"""
        base_name = os.path.splitext(os.path.basename(output_file))[0]
        return os.path.join(self.checkpoint_dir, f"{base_name}_checkpoint.json")
    
    def _save_checkpoint(self, checkpoint_path: str, data: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _load_checkpoint(self, checkpoint_path: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            try:
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None
    
    def _delete_checkpoint(self, checkpoint_path: str):
        """åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if os.path.exists(checkpoint_path):
            try:
                os.remove(checkpoint_path)
            except Exception as e:
                print(f"åˆ é™¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def convert_sft_dataset_to_dpo_optimized(
        self,
        sft_file_path: str,
        output_file: str,
        concurrency: int = 1,
        resume_conversion: bool = True,
        save_interval: int = 5
    ) -> List[Dict[str, str]]:
        """
        ä¼˜åŒ–ç‰ˆæœ¬çš„SFTåˆ°DPOè½¬æ¢ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå†…å­˜ä¼˜åŒ–
        
        Args:
            sft_file_path: SFTæ•°æ®é›†æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºDPOæ•°æ®é›†æ–‡ä»¶è·¯å¾„
            concurrency: å¹¶å‘è¯·æ±‚æ•°
            resume_conversion: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
            save_interval: ä¿å­˜é—´éš”ï¼ˆæ¯è½¬æ¢å¤šå°‘ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ï¼‰
            
        Returns:
            è½¬æ¢åçš„DPOæ•°æ®é›†
        """
        # åŠ è½½SFTæ•°æ®é›†
        with open(sft_file_path, 'r', encoding='utf-8') as f:
            sft_data = json.load(f)
        
        if not isinstance(sft_data, list):
            raise ValueError("SFTæ•°æ®é›†å¿…é¡»æ˜¯JSONæ•°ç»„æ ¼å¼")
        
        checkpoint_path = self._get_checkpoint_path(output_file)
        start_index = 0
        dpo_data = []
        
        # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
        if resume_conversion:
            checkpoint = self._load_checkpoint(checkpoint_path)
            if checkpoint:
                start_index = checkpoint.get('completed_count', 0)
                dpo_data = checkpoint.get('converted_data', [])
                
                if st is not None:
                    st.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è½¬æ¢ï¼Œå·²å®Œæˆ {start_index}/{len(sft_data)} ä¸ªæ ·æœ¬")
                print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è½¬æ¢ï¼Œå·²å®Œæˆ {start_index}/{len(sft_data)} ä¸ªæ ·æœ¬")
        
        # å¦‚æœå·²ç»å…¨éƒ¨å®Œæˆï¼Œç›´æ¥è¿”å›
        if start_index >= len(sft_data):
            if st is not None:
                st.success("âœ… è½¬æ¢å·²å®Œæˆï¼Œç›´æ¥åŠ è½½ç»“æœ")
            return dpo_data
        
        # ç»§ç»­è½¬æ¢å‰©ä½™çš„æ•°æ®
        remaining_data = sft_data[start_index:]
        
        try:
            if concurrency > 1:
                new_dpo_data = self._convert_concurrent_optimized(
                    remaining_data, start_index, len(sft_data), 
                    checkpoint_path, save_interval, concurrency
                )
            else:
                new_dpo_data = self._convert_sequential_optimized(
                    remaining_data, start_index, len(sft_data),
                    checkpoint_path, save_interval
                )
            
            # åˆå¹¶æ•°æ®
            dpo_data.extend(new_dpo_data)
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(dpo_data, f, ensure_ascii=False, indent=2)
            
            # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
            self._delete_checkpoint(checkpoint_path)
            
            if st is not None:
                st.success(f"ğŸ‰ è½¬æ¢å®Œæˆï¼å…±è½¬æ¢ {len(dpo_data)} ä¸ªæ ·æœ¬")
            
            return dpo_data
            
        except Exception as e:
            # å‘ç”Ÿé”™è¯¯æ—¶ä¿å­˜å½“å‰è¿›åº¦
            if dpo_data:
                checkpoint_data = {
                    'completed_count': len(dpo_data),
                    'converted_data': dpo_data,
                    'error': str(e),
                    'timestamp': time.time()
                }
                self._save_checkpoint(checkpoint_path, checkpoint_data)
                
                if st is not None:
                    st.error(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                    st.info(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦åˆ°æ£€æŸ¥ç‚¹ï¼Œä¸‹æ¬¡å¯ä»¥ç»§ç»­è½¬æ¢")
            
            raise e
    
    def _convert_sequential_optimized(
        self, 
        sft_data: List[Dict[str, Any]], 
        start_index: int, 
        total_count: int,
        checkpoint_path: str, 
        save_interval: int
    ) -> List[Dict[str, str]]:
        """
        ä¼˜åŒ–çš„ä¸²è¡Œè½¬æ¢
        """
        dpo_data = []
        
        # åˆ›å»ºStreamlitè¿›åº¦æ¡
        progress_bar = None
        status_text = None
        if st is not None:
            progress_bar = st.progress(start_index / total_count)
            status_text = st.empty()
        
        for i, sft_sample in enumerate(tqdm(sft_data, desc="è½¬æ¢SFTåˆ°DPO", initial=start_index, total=total_count)):
            try:
                current_index = start_index + i
                
                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar is not None:
                    progress = (current_index + 1) / total_count
                    progress_bar.progress(progress)
                    status_text.text(f"è½¬æ¢SFTåˆ°DPO: {current_index + 1}/{total_count} ({progress:.1%})")
                
                # è½¬æ¢å•ä¸ªæ ·æœ¬
                dpo_sample = self.convert_sft_sample_to_dpo(sft_sample)
                dpo_data.append(dpo_sample)
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if (i + 1) % save_interval == 0:
                    checkpoint_data = {
                        'completed_count': current_index + 1,
                        'converted_data': dpo_data,
                        'timestamp': time.time()
                    }
                    self._save_checkpoint(checkpoint_path, checkpoint_data)
                    
                    if st is not None:
                        st.info(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {current_index + 1}/{total_count}")
                
            except Exception as e:
                print(f"è½¬æ¢æ ·æœ¬ {start_index + i} æ—¶å‡ºé”™: {e}")
                continue
        
        return dpo_data
    
    def _convert_concurrent_optimized(
        self, 
        sft_data: List[Dict[str, Any]], 
        start_index: int, 
        total_count: int,
        checkpoint_path: str, 
        save_interval: int, 
        concurrency: int
    ) -> List[Dict[str, str]]:
        """
        ä¼˜åŒ–çš„å¹¶å‘è½¬æ¢
        """
        dpo_data = [None] * len(sft_data)
        completed_count = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = None
        status_text = None
        if st is not None:
            progress_bar = st.progress(start_index / total_count)
            status_text = st.empty()
        
        def convert_single_sample(index: int, sft_sample: Dict[str, Any]) -> tuple:
            try:
                dpo_sample = self.convert_sft_sample_to_dpo(sft_sample)
                return index, dpo_sample
            except Exception as e:
                print(f"è½¬æ¢æ ·æœ¬ {start_index + index} æ—¶å‡ºé”™: {e}")
                return index, None
        
        # å¹¶å‘è½¬æ¢
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            future_to_index = {
                executor.submit(convert_single_sample, i, sft_sample): i 
                for i, sft_sample in enumerate(sft_data)
            }
            
            for future in as_completed(future_to_index):
                index, dpo_sample = future.result()
                if dpo_sample is not None:
                    dpo_data[index] = dpo_sample
                
                completed_count += 1
                current_total = start_index + completed_count
                
                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar is not None:
                    progress = current_total / total_count
                    progress_bar.progress(progress)
                    status_text.text(f"å¹¶å‘è½¬æ¢SFTåˆ°DPO: {current_total}/{total_count} ({progress:.1%})")
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if completed_count % save_interval == 0:
                    # è¿‡æ»¤æ‰Noneå€¼
                    valid_data = [sample for sample in dpo_data if sample is not None]
                    checkpoint_data = {
                        'completed_count': start_index + len(valid_data),
                        'converted_data': valid_data,
                        'timestamp': time.time()
                    }
                    self._save_checkpoint(checkpoint_path, checkpoint_data)
                    
                    if st is not None:
                        st.info(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {current_total}/{total_count}")
        
        # è¿‡æ»¤æ‰Noneå€¼ï¼Œä¿æŒé¡ºåº
        return [sample for sample in dpo_data if sample is not None]
    
    def convert_folder_sft_to_dpo_optimized(
        self,
        sft_folder_path: str,
        output_folder: str,
        concurrency: int = 1,
        resume_conversion: bool = True,
        save_interval: int = 5
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–ç‰ˆæœ¬çš„æ‰¹é‡æ–‡ä»¶å¤¹è½¬æ¢
        """
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        sft_files = []
        for file_name in os.listdir(sft_folder_path):
            if file_name.endswith('.json'):
                sft_files.append(os.path.join(sft_folder_path, file_name))
        
        if not sft_files:
            raise ValueError(f"åœ¨æ–‡ä»¶å¤¹ {sft_folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
        os.makedirs(output_folder, exist_ok=True)
        
        conversion_results = []
        total_converted = 0
        
        for sft_file in sft_files:
            try:
                file_name = os.path.basename(sft_file)
                output_file = os.path.join(output_folder, f"dpo_{file_name}")
                
                if st is not None:
                    st.info(f"ğŸ”„ æ­£åœ¨è½¬æ¢æ–‡ä»¶: {file_name}")
                
                # ä½¿ç”¨ä¼˜åŒ–è½¬æ¢æ–¹æ³•
                dpo_data = self.convert_sft_dataset_to_dpo_optimized(
                    sft_file, output_file, concurrency, resume_conversion, save_interval
                )
                
                conversion_results.append({
                    'input_file': sft_file,
                    'output_file': output_file,
                    'converted_count': len(dpo_data)
                })
                
                total_converted += len(dpo_data)
                
            except Exception as e:
                print(f"è½¬æ¢æ–‡ä»¶ {sft_file} æ—¶å‡ºé”™: {e}")
                continue
        
        return {
            'conversion_results': conversion_results,
            'total_converted': total_converted,
            'total_files': len(conversion_results)
        }