# -*- coding: utf-8 -*-
"""
‰ºòÂåñÁöÑSFTÂà∞DPOÊï∞ÊçÆÈõÜËΩ¨Êç¢Âô®
ÊîØÊåÅÊñ≠ÁÇπÁª≠‰º†ÂíåÂÜÖÂ≠ò‰ºòÂåñÂäüËÉΩ
"""

import json
import os
import time
from typing import List, Dict, Any, Optional
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

from .sft_to_dpo_converter import SFTToDPOConverter
from ..data_loader import DataLoader
from ..model_caller import ModelCaller

# Â∞ùËØïÂØºÂÖ•streamlitÔºåÂ¶ÇÊûú‰∏çÂèØÁî®Âàô‰ΩøÁî®None
try:
    import streamlit as st
except ImportError:
    st = None


class OptimizedSFTToDPOConverter(SFTToDPOConverter):
    """
    ‰ºòÂåñÁöÑSFTÂà∞DPOÊï∞ÊçÆÈõÜËΩ¨Êç¢Âô®
    
    Âú®ÂéüÊúâËΩ¨Êç¢Âô®Âü∫Á°Ä‰∏äÂ¢ûÂä†Ôºö
    - Êñ≠ÁÇπÁª≠‰º†ÂäüËÉΩ
    - ÂÜÖÂ≠ò‰ºòÂåñÔºàÂÆûÊó∂‰øùÂ≠òÔºâ
    - Êõ¥ËØ¶ÁªÜÁöÑËøõÂ∫¶Ë∑üË∏™
    - ÈîôËØØÊÅ¢Â§çÊú∫Âà∂
    """
    
    def __init__(
        self,
        model_caller: ModelCaller,
        data_loader: DataLoader,
        rejected_prompt: str,
        checkpoint_dir: str = "checkpoints",
        sample_min: int = 3,
        sample_max: int = 6
    ):
        """
        ÂàùÂßãÂåñ‰ºòÂåñÁöÑSFTÂà∞DPOËΩ¨Êç¢Âô®
        
        Args:
            model_caller: Ê®°ÂûãË∞ÉÁî®Âô®
            data_loader: Êï∞ÊçÆÂä†ËΩΩÂô®
            rejected_prompt: ÁîüÊàêrejectedÁöÑÊèêÁ§∫Ê®°Êùø
            checkpoint_dir: Ê£ÄÊü•ÁÇπ‰øùÂ≠òÁõÆÂΩï
            sample_min: ÊúÄÂ∞ëÁ§∫‰æãÊï∞Èáè
            sample_max: ÊúÄÂ§öÁ§∫‰æãÊï∞Èáè
        """
        super().__init__(model_caller, data_loader, rejected_prompt, sample_min, sample_max)
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)
    
    def _get_checkpoint_path(self, output_file: str) -> str:
        """Ëé∑ÂèñÊ£ÄÊü•ÁÇπÊñá‰ª∂Ë∑ØÂæÑ"""
        base_name = os.path.splitext(os.path.basename(output_file))[0]
        return os.path.join(self.checkpoint_dir, f"{base_name}_checkpoint.json")
    
    def _save_checkpoint(self, checkpoint_path: str, data: Dict[str, Any]):
        """‰øùÂ≠òÊ£ÄÊü•ÁÇπ"""
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _load_checkpoint(self, checkpoint_path: str) -> Optional[Dict[str, Any]]:
        """Âä†ËΩΩÊ£ÄÊü•ÁÇπ"""
        if os.path.exists(checkpoint_path):
            try:
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Âä†ËΩΩÊ£ÄÊü•ÁÇπÂ§±Ë¥•: {e}")
        return None
    
    def _delete_checkpoint(self, checkpoint_path: str):
        """Âà†Èô§Ê£ÄÊü•ÁÇπÊñá‰ª∂"""
        if os.path.exists(checkpoint_path):
            try:
                os.remove(checkpoint_path)
            except Exception as e:
                print(f"Âà†Èô§Ê£ÄÊü•ÁÇπÂ§±Ë¥•: {e}")
    
    def convert_sft_dataset_to_dpo_optimized(
        self,
        sft_file_path: str,
        output_file: str,
        concurrency: int = 1,
        resume_conversion: bool = True,
        save_interval: int = 5
    ) -> List[Dict[str, str]]:
        """
        ‰ºòÂåñÁâàÊú¨ÁöÑSFTÂà∞DPOËΩ¨Êç¢ÔºåÊîØÊåÅÊñ≠ÁÇπÁª≠‰º†ÂíåÂÜÖÂ≠ò‰ºòÂåñ
        
        Args:
            sft_file_path: SFTÊï∞ÊçÆÈõÜÊñá‰ª∂Ë∑ØÂæÑ
            output_file: ËæìÂá∫DPOÊï∞ÊçÆÈõÜÊñá‰ª∂Ë∑ØÂæÑ
            concurrency: Âπ∂ÂèëËØ∑Ê±ÇÊï∞
            resume_conversion: ÊòØÂê¶ÂêØÁî®Êñ≠ÁÇπÁª≠‰º†
            save_interval: ‰øùÂ≠òÈó¥ÈöîÔºàÊØèËΩ¨Êç¢Â§öÂ∞ë‰∏™Ê†∑Êú¨‰øùÂ≠ò‰∏ÄÊ¨°Ôºâ
            
        Returns:
            ËΩ¨Êç¢ÂêéÁöÑDPOÊï∞ÊçÆÈõÜ
        """
        # Âä†ËΩΩSFTÊï∞ÊçÆÈõÜ
        with open(sft_file_path, 'r', encoding='utf-8') as f:
            sft_data = json.load(f)
        
        if not isinstance(sft_data, list):
            raise ValueError("SFTÊï∞ÊçÆÈõÜÂøÖÈ°ªÊòØJSONÊï∞ÁªÑÊ†ºÂºè")
        
        checkpoint_path = self._get_checkpoint_path(output_file)
        start_index = 0
        dpo_data = []
        
        # Â∞ùËØï‰ªéÊ£ÄÊü•ÁÇπÊÅ¢Â§ç
        if resume_conversion:
            checkpoint = self._load_checkpoint(checkpoint_path)
            if checkpoint:
                start_index = checkpoint.get('completed_count', 0)
                dpo_data = checkpoint.get('converted_data', [])
                
                if st is not None:
                    st.info(f"üîÑ ‰ªéÊ£ÄÊü•ÁÇπÊÅ¢Â§çËΩ¨Êç¢ÔºåÂ∑≤ÂÆåÊàê {start_index}/{len(sft_data)} ‰∏™Ê†∑Êú¨")
                print(f"‰ªéÊ£ÄÊü•ÁÇπÊÅ¢Â§çËΩ¨Êç¢ÔºåÂ∑≤ÂÆåÊàê {start_index}/{len(sft_data)} ‰∏™Ê†∑Êú¨")
        
        # Â¶ÇÊûúÂ∑≤ÁªèÂÖ®ÈÉ®ÂÆåÊàêÔºåÁõ¥Êé•ËøîÂõû
        if start_index >= len(sft_data):
            if st is not None:
                st.success("‚úÖ ËΩ¨Êç¢Â∑≤ÂÆåÊàêÔºåÁõ¥Êé•Âä†ËΩΩÁªìÊûú")
            return dpo_data
        
        # ÁªßÁª≠ËΩ¨Êç¢Ââ©‰ΩôÁöÑÊï∞ÊçÆ
        remaining_data = sft_data[start_index:]
        
        try:
            if concurrency > 1:
                new_dpo_data = self._convert_concurrent_optimized(
                    remaining_data, start_index, len(sft_data), 
                    checkpoint_path, save_interval, concurrency
                )
            else:
                new_dpo_data = self._convert_sequential_optimized(
                    remaining_data, start_index, len(sft_data),
                    checkpoint_path, save_interval
                )
            
            # ÂêàÂπ∂Êï∞ÊçÆ
            dpo_data.extend(new_dpo_data)
            
            # ‰øùÂ≠òÊúÄÁªàÁªìÊûú
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(dpo_data, f, ensure_ascii=False, indent=2)
            
            # Âà†Èô§Ê£ÄÊü•ÁÇπÊñá‰ª∂
            self._delete_checkpoint(checkpoint_path)
            
            if st is not None:
                st.success(f"üéâ ËΩ¨Êç¢ÂÆåÊàêÔºÅÂÖ±ËΩ¨Êç¢ {len(dpo_data)} ‰∏™Ê†∑Êú¨")
            
            return dpo_data
            
        except Exception as e:
            # ÂèëÁîüÈîôËØØÊó∂‰øùÂ≠òÂΩìÂâçËøõÂ∫¶
            if dpo_data:
                checkpoint_data = {
                    'completed_count': len(dpo_data),
                    'converted_data': dpo_data,
                    'error': str(e),
                    'timestamp': time.time()
                }
                self._save_checkpoint(checkpoint_path, checkpoint_data)
                
                if st is not None:
                    st.error(f"‚ùå ËΩ¨Êç¢ËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {str(e)}")
                    st.info(f"üíæ Â∑≤‰øùÂ≠òËøõÂ∫¶Âà∞Ê£ÄÊü•ÁÇπÔºå‰∏ãÊ¨°ÂèØ‰ª•ÁªßÁª≠ËΩ¨Êç¢")
            
            raise e
    
    def _convert_sequential_optimized(
        self, 
        sft_data: List[Dict[str, Any]], 
        start_index: int, 
        total_count: int,
        checkpoint_path: str, 
        save_interval: int
    ) -> List[Dict[str, str]]:
        """
        ‰ºòÂåñÁöÑ‰∏≤Ë°åËΩ¨Êç¢
        """
        dpo_data = []
        
        # ÂàõÂª∫StreamlitËøõÂ∫¶Êù°
        progress_bar = None
        status_text = None
        if st is not None:
            progress_bar = st.progress(start_index / total_count)
            status_text = st.empty()
        
        for i, sft_sample in enumerate(tqdm(sft_data, desc="ËΩ¨Êç¢SFTÂà∞DPO", initial=start_index, total=total_count)):
            try:
                current_index = start_index + i
                
                # Êõ¥Êñ∞ËøõÂ∫¶Êù°
                if progress_bar is not None:
                    progress = (current_index + 1) / total_count
                    progress_bar.progress(progress)
                    status_text.text(f"ËΩ¨Êç¢SFTÂà∞DPO: {current_index + 1}/{total_count} ({progress:.1%})")
                
                # ËΩ¨Êç¢Âçï‰∏™Ê†∑Êú¨
                dpo_sample = self.convert_sft_sample_to_dpo(sft_sample)
                dpo_data.append(dpo_sample)
                
                # ÂÆöÊúü‰øùÂ≠òÊ£ÄÊü•ÁÇπ
                if (i + 1) % save_interval == 0:
                    checkpoint_data = {
                        'completed_count': current_index + 1,
                        'converted_data': dpo_data,
                        'timestamp': time.time()
                    }
                    self._save_checkpoint(checkpoint_path, checkpoint_data)
                    
                    if st is not None:
                        st.info(f"üíæ Â∑≤‰øùÂ≠òÊ£ÄÊü•ÁÇπ: {current_index + 1}/{total_count}")
                
            except Exception as e:
                print(f"ËΩ¨Êç¢Ê†∑Êú¨ {start_index + i} Êó∂Âá∫Èîô: {e}")
                continue
        
        return dpo_data
    
    def _convert_concurrent_optimized(
        self, 
        sft_data: List[Dict[str, Any]], 
        start_index: int, 
        total_count: int,
        checkpoint_path: str, 
        save_interval: int, 
        concurrency: int
    ) -> List[Dict[str, str]]:
        """
        ‰ºòÂåñÁöÑÂπ∂ÂèëËΩ¨Êç¢
        """
        dpo_data = [None] * len(sft_data)
        completed_count = 0
        
        # ÂàõÂª∫ËøõÂ∫¶Êù°
        progress_bar = None
        status_text = None
        if st is not None:
            progress_bar = st.progress(start_index / total_count)
            status_text = st.empty()
        
        def convert_single_sample(index: int, sft_sample: Dict[str, Any]) -> tuple:
            try:
                dpo_sample = self.convert_sft_sample_to_dpo(sft_sample)
                return index, dpo_sample
            except Exception as e:
                print(f"ËΩ¨Êç¢Ê†∑Êú¨ {start_index + index} Êó∂Âá∫Èîô: {e}")
                return index, None
        
        # Âπ∂ÂèëËΩ¨Êç¢
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            future_to_index = {
                executor.submit(convert_single_sample, i, sft_sample): i 
                for i, sft_sample in enumerate(sft_data)
            }
            
            for future in as_completed(future_to_index):
                index, dpo_sample = future.result()
                if dpo_sample is not None:
                    dpo_data[index] = dpo_sample
                
                completed_count += 1
                current_total = start_index + completed_count
                
                # Êõ¥Êñ∞ËøõÂ∫¶Êù°
                if progress_bar is not None:
                    progress = current_total / total_count
                    progress_bar.progress(progress)
                    status_text.text(f"Âπ∂ÂèëËΩ¨Êç¢SFTÂà∞DPO: {current_total}/{total_count} ({progress:.1%})")
                
                # ÂÆöÊúü‰øùÂ≠òÊ£ÄÊü•ÁÇπ
                if completed_count % save_interval == 0:
                    # ËøáÊª§ÊéâNoneÂÄº
                    valid_data = [sample for sample in dpo_data if sample is not None]
                    checkpoint_data = {
                        'completed_count': start_index + len(valid_data),
                        'converted_data': valid_data,
                        'timestamp': time.time()
                    }
                    self._save_checkpoint(checkpoint_path, checkpoint_data)
                    
                    if st is not None:
                        st.info(f"üíæ Â∑≤‰øùÂ≠òÊ£ÄÊü•ÁÇπ: {current_total}/{total_count}")
        
        # ËøáÊª§ÊéâNoneÂÄºÔºå‰øùÊåÅÈ°∫Â∫è
        return [sample for sample in dpo_data if sample is not None]
    
    def convert_folder_sft_to_dpo_optimized(
        self,
        sft_folder_path: str,
        output_folder: str,
        concurrency: int = 1,
        resume_conversion: bool = True,
        save_interval: int = 5
    ) -> Dict[str, Any]:
        """
        ‰ºòÂåñÁâàÊú¨ÁöÑÊâπÈáèÊñá‰ª∂Â§πËΩ¨Êç¢
        """
        # Ëé∑ÂèñÊâÄÊúâJSONÊñá‰ª∂
        sft_files = []
        for file_name in os.listdir(sft_folder_path):
            if file_name.endswith('.json'):
                sft_files.append(os.path.join(sft_folder_path, file_name))
        
        if not sft_files:
            raise ValueError(f"Âú®Êñá‰ª∂Â§π {sft_folder_path} ‰∏≠Ê≤°ÊúâÊâæÂà∞JSONÊñá‰ª∂")
        
        # ÂàõÂª∫ËæìÂá∫Êñá‰ª∂Â§π
        os.makedirs(output_folder, exist_ok=True)
        
        conversion_results = []
        total_converted = 0
        
        for sft_file in sft_files:
            try:
                file_name = os.path.basename(sft_file)
                output_file = os.path.join(output_folder, f"dpo_{file_name}")
                
                if st is not None:
                    st.info(f"üîÑ Ê≠£Âú®ËΩ¨Êç¢Êñá‰ª∂: {file_name}")
                
                # ‰ΩøÁî®‰ºòÂåñËΩ¨Êç¢ÊñπÊ≥ï
                dpo_data = self.convert_sft_dataset_to_dpo_optimized(
                    sft_file, output_file, concurrency, resume_conversion, save_interval
                )
                
                conversion_results.append({
                    'input_file': sft_file,
                    'output_file': output_file,
                    'converted_count': len(dpo_data)
                })
                
                total_converted += len(dpo_data)
                
            except Exception as e:
                print(f"ËΩ¨Êç¢Êñá‰ª∂ {sft_file} Êó∂Âá∫Èîô: {e}")
                continue
        
        return {
            'conversion_results': conversion_results,
            'total_converted': total_converted,
            'total_files': len(conversion_results)
        }